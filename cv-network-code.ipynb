{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation"
      ],
      "metadata": {
        "id": "xpeGb7D-DPqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"bgDc5Q3QdkWQmxPqwe40\")\n",
        "project = rf.workspace(\"parkinglotdetectionteamcentaurus\").project(\"parking-space-4\")\n",
        "version = project.version(1)\n",
        "dataset = version.download(\"tfrecord\")"
      ],
      "metadata": {
        "id": "Xpr3aDsjXgdL",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-object-detection-api protobuf==3.20.3"
      ],
      "metadata": {
        "id": "SnEDcjNTJdVi",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import MeanSquaredError\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy, BinaryCrossentropy"
      ],
      "metadata": {
        "id": "e1q6mDDRmtTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Architecture of the net"
      ],
      "metadata": {
        "id": "X_YHQkC2DajF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yd7dD6J53P8R"
      },
      "outputs": [],
      "source": [
        "def mbconv(x, expand, out, stride, se_ratio=0.25):\n",
        "    \"\"\"\n",
        "    Mobile Inverted Residual Bottleneck (MBConv) with Squeeze-Excitation.\n",
        "\n",
        "    Args:\n",
        "        x: Input tensor\n",
        "        expand: Expansion factor (1 for no expansion)\n",
        "        out_channels: Output channels\n",
        "        stride: Stride (1 or 2)\n",
        "        se_ratio: Squeeze-Excitation reduction ratio\n",
        "\n",
        "    Returns:\n",
        "        Output tensor\n",
        "    \"\"\"\n",
        "    # MBConv block with squeeze-excitation\n",
        "    in_channels = x.shape[-1]\n",
        "    identity = x\n",
        "\n",
        "    # Expansion phase (1x1 Conv)\n",
        "    if expand > 1:\n",
        "        x = layers.Conv2D(in_channels * expand, 1, padding='same', use_bias=False)(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.ReLU(6.0)(x)\n",
        "\n",
        "    # Depthwise phase (3x3 Conv)\n",
        "    x = layers.DepthwiseConv2D(3, strides=stride, padding='same', use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU(6.0)(x)\n",
        "\n",
        "    # Squeeze-Excitation (optional but recommended for parking spot detection)\n",
        "    if se_ratio:\n",
        "        # Squeeze\n",
        "        squeezed = layers.GlobalAveragePooling2D()(x)\n",
        "        squeezed = layers.Dense(max(1, int(in_channels * se_ratio)),\n",
        "                              activation='relu')(squeezed)\n",
        "        # Excite\n",
        "        squeezed = layers.Dense(in_channels * expand,\n",
        "                              activation='hard_sigmoid')(squeezed)\n",
        "        # Reshape and multiply\n",
        "        x = layers.Multiply()([x, layers.Reshape((1, 1, in_channels * expand))(squeezed)])\n",
        "\n",
        "    # Projection phase (1x1 Conv)\n",
        "    x = layers.Conv2D(out, 1, padding='same', use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # Residual connection if possible\n",
        "    if stride == 1 and in_channels == out:\n",
        "        x = layers.Add()([x, identity])\n",
        "\n",
        "    return x\n",
        "\n",
        "def build_model(input_shape=(640, 640, 3), num_classes=2):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Backbone\n",
        "    x = layers.Conv2D(8, 3, strides=2, padding='same')(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU(6)(x)\n",
        "\n",
        "    x = mbconv(x, expand=1, out=8, stride=1)\n",
        "    tf.print(x.shape)\n",
        "    x = mbconv(x, expand=6, out=16, stride=2)\n",
        "    tf.print(x.shape)\n",
        "    c3 = mbconv(x, expand=6, out=24, stride=2)  # 80x80\n",
        "    tf.print('c3', c3.shape)\n",
        "    c4 = mbconv(c3, expand=6, out=32, stride=2) # 40x40\n",
        "    tf.print('c4', c4.shape)\n",
        "    c5 = mbconv(c4, expand=6, out=48, stride=2) # 20x20\n",
        "    tf.print('c5', c5.shape)\n",
        "\n",
        "\n",
        "    # Detection Heads (Classification + Regression)\n",
        "    outputs = []\n",
        "    for i, f in enumerate([c3, c4, c5]):\n",
        "        # Regression head\n",
        "        reg = layers.GlobalAveragePooling2D()(f)\n",
        "        reg = layers.Dense(100 * 4)(reg)\n",
        "        reg_output = layers.Reshape((100, 4), name=f'reg_{i+1}')(reg)  # <-- NAMED\n",
        "\n",
        "        # Classification head (NEW)\n",
        "        cls = layers.GlobalAveragePooling2D()(f)\n",
        "        cls = layers.Dense(100 * num_classes)(cls)\n",
        "        cls_output = layers.Reshape((100, num_classes), name=f'cls_{i+1}')(cls)  # <-- NAMED\n",
        "\n",
        "        outputs.extend([cls_output, reg_output])  # Order matches loss keys\n",
        "\n",
        "    return Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parsing and prepocessing dataset, learning\n"
      ],
      "metadata": {
        "id": "_fF8-AylpFGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_tfrecord(example_proto):\n",
        "    MAX_OBJECTS = 100  # Set to maximum expected objects per image\n",
        "\n",
        "    feature_description = {\n",
        "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
        "        'image/object/bbox/xmin': tf.io.VarLenFeature(tf.float32),\n",
        "        'image/object/bbox/xmax': tf.io.VarLenFeature(tf.float32),\n",
        "        'image/object/bbox/ymin': tf.io.VarLenFeature(tf.float32),\n",
        "        'image/object/bbox/ymax': tf.io.VarLenFeature(tf.float32),\n",
        "        'image/object/class/label': tf.io.VarLenFeature(tf.int64),\n",
        "    }\n",
        "\n",
        "    example = tf.io.parse_single_example(example_proto, feature_description)\n",
        "\n",
        "    # Image processing\n",
        "    image = tf.image.decode_jpeg(example['image/encoded'], channels=3)\n",
        "    image = tf.image.resize(image, (640, 640))\n",
        "    image = image / 255.0\n",
        "\n",
        "    # Bounding box processing\n",
        "    xmin = tf.sparse.to_dense(example['image/object/bbox/xmin'])\n",
        "    ymin = tf.sparse.to_dense(example['image/object/bbox/ymin'])\n",
        "    xmax = tf.sparse.to_dense(example['image/object/bbox/xmax'])\n",
        "    ymax = tf.sparse.to_dense(example['image/object/bbox/ymax'])\n",
        "    labels = tf.sparse.to_dense(example['image/object/class/label'])\n",
        "    labels = labels - 1\n",
        "\n",
        "    # Convert to center format [x_center, y_center, width, height]\n",
        "    boxes = tf.stack([\n",
        "        (xmin + xmax) / 2.0,\n",
        "        (ymin + ymax) / 2.0,\n",
        "        xmax - xmin,\n",
        "        ymax - ymin\n",
        "    ], axis=-1)\n",
        "\n",
        "    # [FIX] Pad boxes and labels to fixed size\n",
        "    boxes = tf.pad(boxes, [[0, MAX_OBJECTS - tf.shape(boxes)[0]], [0, 0]])\n",
        "    labels = tf.pad(labels, [[0, MAX_OBJECTS - tf.shape(labels)[0]]])\n",
        "\n",
        "    # [FIX] Set fixed shapes\n",
        "    boxes.set_shape([MAX_OBJECTS, 4])\n",
        "    labels.set_shape([MAX_OBJECTS])\n",
        "\n",
        "    # Format targets for 3 detection heads\n",
        "    targets = {}\n",
        "    for i in range(3):  # For 3 detection heads\n",
        "        targets[f'cls_{i+1}'] = labels\n",
        "        targets[f'reg_{i+1}'] = boxes\n",
        "\n",
        "    return image, targets\n",
        "\n",
        "def create_dataset(tfrecord_path, batch_size=8):\n",
        "    \"\"\"Create dataset with padded batching\"\"\"\n",
        "    dataset = tf.data.TFRecordDataset(tfrecord_path)\n",
        "    dataset = dataset.map(parse_tfrecord)\n",
        "\n",
        "    # [MODIFIED] Use padded_batch instead of regular batch\n",
        "    return dataset.repeat().padded_batch(\n",
        "        batch_size,\n",
        "        padding_values=(\n",
        "            tf.constant(0.0, dtype=tf.float32),  # Image padding\n",
        "            {                                     # Target padding\n",
        "                'cls_1': tf.constant(-1, dtype=tf.int64),\n",
        "                'reg_1': tf.constant(0.0, dtype=tf.float32),\n",
        "                'cls_2': tf.constant(-1, dtype=tf.int64),\n",
        "                'reg_2': tf.constant(0.0, dtype=tf.float32),\n",
        "                'cls_3': tf.constant(-1, dtype=tf.int64),\n",
        "                'reg_3': tf.constant(0.0, dtype=tf.float32)\n",
        "            }\n",
        "        )\n",
        "    ).prefetch(tf.data.AUTOTUNE)\n",
        "class MaskedSparseCategoricalCrossentropy(tf.keras.losses.Loss):\n",
        "    def __init__(self, num_classes=2, from_logits=True, name='masked_sparse_cce', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.num_classes = num_classes\n",
        "        self.from_logits = from_logits\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "\n",
        "        batch_size = tf.shape(y_pred)[0]\n",
        "        y_pred = tf.reshape(y_pred, [batch_size, -1, self.num_classes])\n",
        "        y_true = tf.reshape(y_true, [batch_size, -1])\n",
        "\n",
        "        mask = tf.not_equal(y_true, -1)\n",
        "        loss = tf.keras.losses.sparse_categorical_crossentropy(\n",
        "            y_true, y_pred, from_logits=self.from_logits\n",
        "        )\n",
        "        return tf.reduce_mean(tf.boolean_mask(loss, mask))\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'num_classes': self.num_classes,  # Still include in config\n",
        "            'from_logits': self.from_logits\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        # Handle both old and new config versions\n",
        "        return cls(\n",
        "            num_classes=config.get('num_classes', 2),  # Safe get with default\n",
        "            from_logits=config.get('from_logits', True),\n",
        "            name=config.get('name', 'masked_sparse_cce')\n",
        "        )\n",
        "\n",
        "def main():\n",
        "    # Initialize model\n",
        "    model = build_model(num_classes=2)\n",
        "\n",
        "    # Load dataset\n",
        "    train_dataset = create_dataset(\"/content/parking-space-4-1/train/vacant_space-NFsp-KKS2-NE6S.tfrecord\")\n",
        "    val_dataset = create_dataset(\"/content/parking-space-4-1/valid/vacant_space-NFsp-KKS2-NE6S.tfrecord\")\n",
        "    test_dataset = create_dataset(\"/content/parking-space-4-1/test/vacant_space-NFsp-KKS2-NE6S.tfrecord\")\n",
        "\n",
        "    # Verify dataset\n",
        "    for images, targets in train_dataset.take(1):\n",
        "        print(\"Image shape:\", images.shape)  # Should be (8, 640, 640, 3)\n",
        "        print(\"Class shapes:\", {k: v.shape for k, v in targets.items()})\n",
        "        # Should all be (8, 100)\n",
        "        print(\"Box shapes:\", {k: v.shape for k, v in targets.items()})\n",
        "        # Should all be (8, 100, 4)\n",
        "\n",
        "    # Define losses with masking\n",
        "    losses = {\n",
        "        'cls_1': MaskedSparseCategoricalCrossentropy(num_classes=2),\n",
        "        'reg_1': MeanSquaredError(),\n",
        "        'cls_2': MaskedSparseCategoricalCrossentropy(num_classes=2),\n",
        "        'reg_2': MeanSquaredError(),\n",
        "        'cls_3': MaskedSparseCategoricalCrossentropy(num_classes=2),\n",
        "        'reg_3': MeanSquaredError()\n",
        "    }\n",
        "\n",
        "    loss_weights = {\n",
        "        'cls_1': 1.0, 'reg_1': 1.0,\n",
        "        'cls_2': 1.0, 'reg_2': 1.0,\n",
        "        'cls_3': 1.0, 'reg_3': 1.0\n",
        "    }\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(patience=5, monitor='val_loss', restore_best_weights=True),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3),\n",
        "        ModelCheckpoint('best_model.h5', save_best_only=True)\n",
        "    ]\n",
        "    # Compile model\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=1e-4),\n",
        "        loss=losses,\n",
        "        loss_weights=loss_weights,\n",
        "        metrics={'cls_1': 'accuracy', 'cls_2': 'accuracy', 'cls_3': 'accuracy'}\n",
        "    )\n",
        "    model.summary()\n",
        "    # Train\n",
        "\n",
        "    history = model.fit(\n",
        "        train_dataset,\n",
        "        epochs=20,\n",
        "        steps_per_epoch=100,\n",
        "        validation_data=val_dataset,\n",
        "        validation_steps=50,\n",
        "        callbacks=callbacks\n",
        "    )\n",
        "\n",
        "    print(\"\\\\nFinal Test Evaluation:\")\n",
        "    model.evaluate(test_dataset, steps=50)\n",
        "\n",
        "    print(\"\\\\nValidation Set Evaluation:\")\n",
        "    model.evaluate(val_dataset, steps=50)\n",
        "\n",
        "    # Save\n",
        "    model.save('parking_detector.h5')\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    '''\n",
        "    # Convert to TFLite (quantization optional)\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "    tflite_model = converter.convert()\n",
        "    with open('model.tflite', 'wb') as f:\n",
        "        f.write(tflite_model)\n",
        "    '''"
      ],
      "metadata": {
        "id": "KTq9fu_QrrIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_tflite(model_path, quantize=True):\n",
        "    \"\"\"Converts Keras model to TFLite with float16 quantization\"\"\"\n",
        "    model = tf.keras.models.load_model(\n",
        "        model_path,\n",
        "        custom_objects={'MaskedSparseCategoricalCrossentropy': MaskedSparseCategoricalCrossentropy}\n",
        "    )\n",
        "\n",
        "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "\n",
        "    if quantize:\n",
        "        # Float16 quantization (no representative dataset needed)\n",
        "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "        converter.target_spec.supported_types = [tf.float16]  # Key line\n",
        "    else:\n",
        "        # Basic optimization without quantization\n",
        "        converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]\n",
        "\n",
        "    tflite_model = converter.convert()\n",
        "\n",
        "    ext = 'fp16.tflite' if quantize else 'basic.tflite'\n",
        "    with open(f'parking_model_{ext}', 'wb') as f:\n",
        "        f.write(tflite_model)\n",
        "\n",
        "# In main() after training:\n",
        "\n",
        "\n",
        "# Add test inference to verify conversion\n",
        "def test_tflite_inference():\n",
        "    interpreter = tf.lite.Interpreter(model_path='parking_model_fp16.tflite')\n",
        "    interpreter.allocate_tensors()\n",
        "\n",
        "    # Get input details for camera setup\n",
        "    input_details = interpreter.get_input_details()\n",
        "    output_details = interpreter.get_output_details()\n",
        "\n",
        "    print(\"\\nTFLite Input Requirements:\")\n",
        "    print(f\"Input Shape: {input_details[0]['shape']}\")\n",
        "    print(f\"Input Type: {input_details[0]['dtype']}\")\n",
        "    print(\"\\nTFLite Output Shapes:\")\n",
        "    for out in output_details:\n",
        "        print(f\"{out['name']}: {out['shape']}\")\n",
        "\n",
        "# Call after conversion\n",
        "\n",
        "convert_to_tflite('best_model.h5', quantize=True)\n",
        "test_tflite_inference()"
      ],
      "metadata": {
        "id": "H-x42ifn4q8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def visualize_predictions(model_path, tfrecord_path, num_samples=4):\n",
        "    \"\"\"Visualize dataset samples with ground truth and predictions\"\"\"\n",
        "    # 1. Load model with custom loss handling\n",
        "    model = tf.keras.models.load_model(\n",
        "        model_path,\n",
        "        custom_objects={\n",
        "            'MaskedSparseCategoricalCrossentropy': MaskedSparseCategoricalCrossentropy\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # 2. Create dataset without batching\n",
        "    dataset = tf.data.TFRecordDataset(tfrecord_path).map(parse_tfrecord)\n",
        "\n",
        "    # 3. Process samples\n",
        "    for image, targets in dataset.take(num_samples):\n",
        "        # Convert image and add batch dimension\n",
        "        img_array = tf.expand_dims(image, 0)\n",
        "\n",
        "        # Get predictions\n",
        "        predictions = model.predict(img_array)\n",
        "\n",
        "        # Convert to numpy and denormalize\n",
        "        img_display = (image.numpy() * 255).astype(np.uint8)\n",
        "\n",
        "        # Prepare subplots\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
        "\n",
        "        # Plot ground truth\n",
        "        plot_boxes(ax1, img_display, targets, 'Ground Truth')\n",
        "\n",
        "        # Plot predictions\n",
        "        plot_boxes(ax2, img_display, predictions, 'Predictions')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "def plot_boxes(ax, image, data, title):\n",
        "    \"\"\"Universal plotting function for both ground truth and predictions\"\"\"\n",
        "    ax.imshow(image)\n",
        "    ax.set_title(title)\n",
        "    ax.axis('off')\n",
        "\n",
        "    # Different handling for ground truth vs predictions\n",
        "    if isinstance(data, dict):  # Ground truth\n",
        "        boxes = data['reg_1'].numpy()[0]\n",
        "        labels = data['cls_1'].numpy()[0]\n",
        "        confidences = None\n",
        "    else:  # Predictions\n",
        "        boxes = data[1][0]  # First regression head\n",
        "        labels = tf.argmax(data[0][0], axis=-1).numpy()  # First classification head\n",
        "        confidences = tf.reduce_max(tf.nn.softmax(data[0][0]), axis=-1).numpy()\n",
        "\n",
        "    # Convert boxes to pixel coordinates\n",
        "    valid_mask = labels != -1\n",
        "    boxes = boxes[valid_mask]\n",
        "    print('boxes: ', boxes)\n",
        "    labels = labels[valid_mask]\n",
        "    print('labels: ',labels)\n",
        "    if confidences is not None:\n",
        "        confidences = confidences[valid_mask]\n",
        "\n",
        "    # Convert from center to corner format\n",
        "    x_center, y_center, width, height = boxes.T * 1280  # Scale to image size\n",
        "    xmin = x_center - width/2\n",
        "    ymin = y_center - height/2\n",
        "\n",
        "    print('xmin :', xmin, '\\nymin: ', ymin, '\\nwidth: ', '\\nheight: ', height)\n",
        "\n",
        "    # Plot boxes\n",
        "    for i, (x, y, w, h) in enumerate(zip(xmin, ymin, width, height)):\n",
        "        color = 'green' if labels[i] == 1 else 'red'  # Assuming 0=free, 1=occupied\n",
        "        rect = plt.Rectangle((x, y), w, h, fill=False, linewidth=1.5, edgecolor=color)\n",
        "        ax.add_patch(rect)\n",
        "\n",
        "        label_text = f'{\"Free\" if labels[i] == 1 else \"Occupied\"}'\n",
        "        if confidences is not None:\n",
        "            label_text += f' ({confidences[i]:.2f})'\n",
        "        ax.text(x, y-5, label_text, color=color, fontsize=9,\n",
        "                bbox=dict(facecolor='white', alpha=0.7, edgecolor='none'))\n",
        "\n",
        "# Usage\n",
        "visualize_predictions(\n",
        "    model_path='/content/best_model.h5',\n",
        "    tfrecord_path='/content/parking-space-4-1/test/vacant_space-NFsp-KKS2-NE6S.tfrecord',\n",
        "    num_samples=2\n",
        ")"
      ],
      "metadata": {
        "id": "grmo9lyc1Rqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vallidating and Testing"
      ],
      "metadata": {
        "id": "6LsYrJcdDokY"
      }
    }
  ]
}